# huggingface 基础
本章节主要对 ai 开发平台 huggingface 做简要介绍，和使用说明，主要内容包含:  
1. huggingface 的平台简介  
1. huggingface 平台的模型使用的前置准备工作  
1. gpt2 推理案例  

## 平台简介
1. huggingface 是一个 模型/数据集 托管平台，其上托管了各类 ai 模型  
1. huggingface 对 nlp 领域做了很多工作，对于主流的 nlp 模型的 使用/训练 均有较好的支持  
1. 其中 transformers 库封装了诸多常用操作，相比于直接使用 pytorch 开发，使用 transformers 可以节省不少时间  
1. 当然 huggingface 上也有一些非 nlp 方向，只是单纯上传到了 huggingface 平台的模型(相当于把 huggingface 当成 github 使用)  

## 前置准备工作
在正式使用 huggingface 前，我们需要做以下准备工作:  
1. 翻墙代理 - huggingface 被墙了，其模型的下载需要有代理支持  
1. git lfs 的开启 - 建议使用 git 的方式下载模型，需要开启 git 的大文件支持  

备注:  
1. 在 git 安装好了后，执行 `git lfs install` 即可开启 git 的大文件下载功能  
1. 切换到 models 目录，执行以下指令，下载后续 gpt2 推理时需要用到的模型:  
```bash
# 需要注意的是，这里的 http.proxy 为翻墙代理，依据自己的代理状态做配置调整
git clone -c http.proxy="http://192.168.101.216:7890" https://huggingface.co/wujohns/gpt2-chitchat-learn
```

## gpt2 推理案例
详细代码可以参考 [c3-hugging_face_base/gpt2-infer.ipynb](/c3-hugging_face_base/gpt2-infer.ipynb)  

### gpt2 模型的训练与推理简述
在具体分析 gpt2 的代码案例之前，这里使用两张图解简单介绍一下 gpt2 模型是如何训练与推理的:  

训练:  
![c3-hugging_face_base/train.png](/c3-hugging_face_base/train.png)  

推理:  
![c3-hugging_face_base/infer.png](/c3-hugging_face_base/infer.png)  

这些信息的情报来源:  
1. 硬抗一波流: 需要系统学习 nlp 相关算法知识(rnn -> lstm -> attention -> transformer -> gpt2)，同时查阅相关的代码实现，对其机制理解后可以定制化的修改模型  
1. 例如其中部分相关源码实现可以参考: [huggingface gpt2](https://huggingface.co/docs/transformers/v4.35.0/en/model_doc/gpt2#transformers.GPT2LMHeadModel.forward)  
1. 退而求其次: 依据其实现范式和搜索到的其他人的整理，大致理解其计算策略，在实际工程中引入时仅作细微调整和适配  

### 模型加载过程说明
```py
# 加载词库
tokenizer = AutoTokenizer.from_pretrained(
  pretrained_model_name_or_path=getAbsPath('../models/gpt2-chitchat-learn/')
)

# 加载模型(两种方式都行，均为在本地加载，模型目录事先通过 git clone 到本地，相比于直接使用 name 的方式可以减少模型加载的远端检查时间)
model = AutoModelForCausalLM.from_pretrained(
  pretrained_model_name_or_path=getAbsPath('../models/gpt2-chitchat-learn/')
)

model = model.to(device)
model.eval()
```

说明:  
1. 这里使用 transformers.AutoTokenizer 从下载的 huggingface 模型中加载对应的编码器  
1. 编码器的作用是将文字转换为对应的 token 并在后续转换为可供模型执行计算的张量  
1. 这里使用 transformers.AutoModelForCausalLM 加载对应的模型  
1. 加载模型时使用 .to(device) 将模型加载到指定的设备  
1. 同时使用 .eval() 确保在执行模型推理时，参数被锁定，即为推理模式，而非训练模式  

### 模型运行过程说明
```py
# 模型运行结果的处理策略配置(topk_topp策略)
temperature = 1     # 生成温度, 设置为1即保持原有系数不变
topk = 10           # 最高k选1
topp = 0.8          # 最高累积概率(这里设置为0, 即最终只取了概率最高的词，建议修改为其他值)
repetition_penalty = 1.0    # 重复惩罚系数，这里设置为 1.0 即保持原有概率

# 语句与记忆策略
max_len = 25            # 单条响应语句的最大文字数目
max_history_len = 3     # 关联的最大上下文条目(TODO: 后续的定制化改造中尝试采用总结式AI做记忆压缩)

# 生成逻辑核心部分
def generate_response (input_ids):
  # 变为张量格式并进行扩维
  input_ids = torch.LongTensor([input_ids]).to(device)

  # 回复记录
  response = []

  # 按照最大字数生成内容
  for i in range(max_len):
    input_ids = model.generate(
      input_ids=input_ids,
      do_sample=True,           # 这个参数被设定后续的配置才会生效

      temperature=temperature,
      top_k=topk,
      top_p=topp,
      repetition_penalty=repetition_penalty,

      bos_token_id=tokenizer.cls_token_id,
      pad_token_id=tokenizer.pad_token_id,
      eos_token_id=tokenizer.pad_token_id,

      max_new_tokens=1
    )
    next_token = input_ids[:, -1][0]

    # 如果词是 [SEP] 则表明该轮 response 结束
    if next_token == tokenizer.sep_token_id:
      break
    response.append(next_token.item())
  
  response_str = tokenizer.convert_ids_to_tokens(response)
  response_str = ''.join(response_str)
  return response_str

# 生成逻辑处理
def generate(history):
  input_ids = [tokenizer.cls_token_id]   # 每个 input 以 [CLS] 为开头

  # 历史记录拼接
  for history_id, history_str in enumerate(history):
    history_str_ids = tokenizer.encode(history_str, add_special_tokens=False)
    input_ids.extend(history_str_ids)
    input_ids.append(tokenizer.sep_token_id)

  res = generate_response(input_ids)
  print(res)

# mock 的历史记录
history = [
  '你是谁',
  '我是猫雷',
  '好的猫雷，可以唱一首“威风堂堂”吗？',
  '小拳拳'
]
history.append('喵露露要反击了')        # 将当前的 text 拼接到历史记录中
history = history[-max_history_len:]   # 记忆长度控制


generate(history)
```

说明:  
1. 在该阶段的学习中可以先暂时忽略 temperature/topk/topp/repetition_penalty 的生效原理，只需要确认其对生成的作用影响即可  
1. 这里采用的是多轮对话模式  
