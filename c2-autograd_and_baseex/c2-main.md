# 自动求导与基础案例
本章节主要就 ai 开发的基础原理，以及在该原理上当前生态的工程使用做详细说明，主要内容包含:  
1. 导数运算在 ai 训练中的运作流程以及作用  
1. 张量(Tensor)的相关介绍  
1. pytorch 反向传播的简单展示  
1. pytorch 实践项目，以及其工程上的计算图机制  

## 导数运算与训练流程
求导在 ai 训练中扮演了关键角色，特别是在深度学习领域。这里通过一个简单的求导案例，简要说明训练流程:  
1. 目前有数据 data_x 与 data_y，期望可以通过 data_x 预测 data_y  
1. 同时指定损失计算方法为 `loss = (预测值 - 实际值)^2` 
1. 现建立一个模型只有一个参数 w，将其模型计算定义为 `y = w * x`，并将 w 的初始值设定为 start_w  
1. 基于该模型计算的预测值为 `infer_y = start_w * data_x`  
1. `loss = (infer_y - data_y)^2`  
1. 损失函数相对于 infer_y 的导数为 `dL/dy = 2 * (infer_y - data_y)`  
1. y 相对于 w 的导数为 `dy/dw = x`
1. 通过 [链式法则](/c2-autograd_and_baseex/c2-grad-chain.md) 可以得知：损失函数相对于 w 的导数为 `dL/dw = (dL/dy) * (dy/dw) = 2 * (infer_y - data_y) * data_x`  

上述的 dL/dw 即权重(参数) w 对 loss 的梯度，利用该梯度我们可以去调整参数 w 以实现降低 loss 的目标，例如(这里使用简单的梯度下降算法):  
`new_w = start_w - 学习率 * dL/dw`

重复这样的过程，即可实现对模型参数的训练，这里也有一个简单的图解:  
![train.png](/c2-autograd_and_baseex/train.png)  

## 张量(Tensor)
张量是一种多为数组，是深度学习中最基本的数据结构之一，其主要有以下的作用:  
1. 用于 描述样本数据 与 模型参数    
1. 在框架中封装了针对该类型的方法便于使用 GPU 设备处理处理  
1. 便于计算图的追溯，以实现自动微分从计算对应参数的梯度(求导)  

## pytorch 反向传播基础演示


## pytorch 实践项目

