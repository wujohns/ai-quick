{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pyEnvs\\ai-quick\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2LMHeadModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(13317, 768)\n",
       "        (wpe): Embedding(300, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-9): 10 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): Linear(\n",
       "                in_features=768, out_features=2304, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (c_proj): Conv1D()\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=13317, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 预先准备\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# 使用的设备\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "# 绝对路径获取方法\n",
    "# curPath = os.path.dirname(os.path.abspath(__file__))\n",
    "# 在 jupyter 中无法获取到 __file__, 但是可以肯定的是其运行路径是文件所在目录\n",
    "curPath = os.getcwd()\n",
    "def getAbsPath (relativePath):\n",
    "  joinPath = os.path.join(curPath, relativePath)\n",
    "  return os.path.normpath(\n",
    "    os.path.abspath(joinPath)\n",
    "  )\n",
    "\n",
    "# 加载词库\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  pretrained_model_name_or_path=getAbsPath('../models/gpt2-chitchat-learn/')\n",
    ")\n",
    "\n",
    "# 加载模型(两种方式都行，均为在本地加载，模型目录事先通过 git clone 到本地，相比于直接使用 name 的方式可以减少模型加载的远端检查时间)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  pretrained_model_name_or_path=getAbsPath('../models/gpt2-chitchat-learn/')\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "  model,\n",
    "  getAbsPath('../models/gpt2-chitchat-lora/checkpoint-20')\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "反击就是对自己的猫嗨，我不敢跟它讲话，毕竟我怕猫，\n"
     ]
    }
   ],
   "source": [
    "# 模型运行结果的处理策略配置(topk_topp策略)\n",
    "temperature = 1     # 生成温度, 设置为1即保持原有系数不变\n",
    "topk = 10           # 最高k选1\n",
    "topp = 0.8          # 最高累积概率(这里设置为0, 即最终只取了概率最高的词，建议修改为其他值)\n",
    "repetition_penalty = 1.0    # 重复惩罚系数，这里设置为 1.0 即保持原有概率\n",
    "\n",
    "# 语句与记忆策略\n",
    "max_len = 25            # 单条响应语句的最大文字数目\n",
    "max_history_len = 3     # 关联的最大上下文条目(TODO: 后续的定制化改造中尝试采用总结式AI做记忆压缩)\n",
    "\n",
    "# 生成逻辑核心部分\n",
    "def generate_response (input_ids):\n",
    "  # 变为张量格式并进行扩维\n",
    "  input_ids = torch.LongTensor([input_ids]).to(device)\n",
    "\n",
    "  # 回复记录\n",
    "  response = []\n",
    "\n",
    "  # 按照最大字数生成内容\n",
    "  for i in range(max_len):\n",
    "    input_ids = model.generate(\n",
    "      input_ids=input_ids,\n",
    "      do_sample=True,           # 这个参数被设定后续的配置才会生效\n",
    "\n",
    "      temperature=temperature,\n",
    "      top_k=topk,\n",
    "      top_p=topp,\n",
    "      repetition_penalty=repetition_penalty,\n",
    "\n",
    "      bos_token_id=tokenizer.cls_token_id,\n",
    "      pad_token_id=tokenizer.pad_token_id,\n",
    "      eos_token_id=tokenizer.pad_token_id,\n",
    "\n",
    "      max_new_tokens=1\n",
    "    )\n",
    "    next_token = input_ids[:, -1][0]\n",
    "\n",
    "    # 如果词是 [SEP] 则表明该轮 response 结束\n",
    "    if next_token == tokenizer.sep_token_id:\n",
    "      break\n",
    "    response.append(next_token.item())\n",
    "  \n",
    "  response_str = tokenizer.convert_ids_to_tokens(response)\n",
    "  response_str = ''.join(response_str)\n",
    "  return response_str\n",
    "\n",
    "# 生成逻辑处理\n",
    "def generate(history):\n",
    "  input_ids = [tokenizer.cls_token_id]   # 每个 input 以 [CLS] 为开头\n",
    "\n",
    "  # 历史记录拼接\n",
    "  for history_id, history_str in enumerate(history):\n",
    "    history_str_ids = tokenizer.encode(history_str, add_special_tokens=False)\n",
    "    input_ids.extend(history_str_ids)\n",
    "    input_ids.append(tokenizer.sep_token_id)\n",
    "\n",
    "  res = generate_response(input_ids)\n",
    "  print(res)\n",
    "\n",
    "# mock 的历史记录\n",
    "history = [\n",
    "  '你是谁',\n",
    "  '我是猫雷',\n",
    "  '好的猫雷，可以唱一首“威风堂堂”吗？',\n",
    "  '小拳拳'\n",
    "]\n",
    "history.append('喵露露要反击了')        # 将当前的 text 拼接到历史记录中\n",
    "history = history[-max_history_len:]   # 记忆长度控制\n",
    "\n",
    "\n",
    "generate(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-quick",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
